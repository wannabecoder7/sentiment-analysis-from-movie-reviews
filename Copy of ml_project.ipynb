{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1esd248ccNA2O-ernzhb-DOokObBIvJ4o","timestamp":1719314559198}],"authorship_tag":"ABX9TyNb6QXqoXqVQTL40GV7Hxmb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import re\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import nltk\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Download NLTK data\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt')  # Download the 'punkt' tokenizer\n","\n","# Load the dataset\n","data = pd.read_csv('/test.csv')\n","\n","\n","# Adjust column name if necessary\n","review_column = 'text'  # Update this if the actual column name is different\n","sentiment_column = 'label'  # Update this if the actual column name is different\n","\n","# Preprocessing function\n","def preprocess_text(text):\n","    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n","    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n","    text = text.lower()  # Convert to lowercase\n","    tokens = nltk.word_tokenize(text)  # Tokenization\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stopwords.words('english')]\n","    return ' '.join(tokens)\n","\n","# Apply preprocessing\n","data['cleaned_review'] = data[review_column].apply(preprocess_text)\n","\n","# Vectorize the text using TF-IDF\n","vectorizer = TfidfVectorizer(max_features=1000)\n","X = vectorizer.fit_transform(data['cleaned_review']).toarray()\n","\n","# Encode the sentiment labels\n","label_encoder = LabelEncoder()\n","y = label_encoder.fit_transform(data['label'])\n","\n","# Combine TF-IDF vectors with sentiment labels for clustering\n","features = pd.DataFrame(X)\n","features.columns = [str(i) for i in features.columns]\n","\n","# Apply K-means clustering\n","kmeans = KMeans(n_clusters=3, random_state=42)  # Assuming 3 clusters\n","kmeans.fit(features)\n","\n","# Assign cluster labels to each review\n","data['cluster'] = kmeans.labels_\n","\n","# Save the clustered data\n","data.to_csv('clustered_imdb_reviews.csv', index=False)\n","\n","# Optional: Print the resulting clusters\n","print(data[['text', 'label', 'cluster']].head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FneD23UgFF4t","executionInfo":{"status":"ok","timestamp":1719314533122,"user_tz":-330,"elapsed":6390,"user":{"displayName":"Shikhardeep","userId":"02967527948886856412"}},"outputId":"a25c7a88-34ad-4860-8f5d-a585b9decde8"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["                                                text     label  cluster\n","0  lovingly photographed in the manner of a golde...  positive        1\n","1              consistently clever and suspenseful .  positive        1\n","2  it's like a \" big chill \" reunion of the baade...  positive        1\n","3  the story gives ample opportunity for large-sc...  positive        1\n","4                  red dragon \" never cuts corners .  positive        1\n"]}]}]}